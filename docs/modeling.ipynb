{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66c34304",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "This notebook will handle everything related to modeling the data to fit into the Kaggle competition. For this, we have a simple workflow. We start by loading the training data and fit several models to it. Afterwards, we apply these trained models to the test data and evaluate their results, choosing the best one and submitting it to Kaggle. \n",
    "\n",
    "The criteria for evaluation will be the Area Under the Curve (AUC)\n",
    "\n",
    "Let's get this started then. We start by handling all the imports necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4edcca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will handle all the imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt   \n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, TimeSeriesSplit, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcde4877",
   "metadata": {},
   "source": [
    "We can now start loading the actual data that will be used in the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3796811a",
   "metadata": {},
   "source": [
    "# TODO: Add Loading of Testing and Training data, and separate them into features and targets, and then apply SMOTE to the train classes and test_train splitting. Also, order them by time and drop all the string things that aren't necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d94d5bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../data/train.csv\", sep=\",\")\n",
    "test_data = pd.read_csv(\"../data/test.csv\", sep=\",\")\n",
    "\n",
    "unwanted = [\"status\", \"loan_id\"]\n",
    "features = [x for x in list(train_data) if x not in unwanted]\n",
    "target = [\"status\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0c08bd",
   "metadata": {},
   "source": [
    "# Training a Model and finding the best parameters\n",
    "\n",
    "With the data loaded, we can start trying to find the best model to predict the results. To find the best hyperparameters for each model, we will use Grid Searching, as randomized searches would not yield any better results, and evaluate the results using the AUC metric. As for splitting, we will use the `TimeSeriesSplit`, as it splits the data taking in account timed events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8e52150",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLITTER = TimeSeriesSplit()\n",
    "\n",
    "# Don't forget that here go the training X and y split values from test_train_split\n",
    "\n",
    "def train_model_grid(model, params, cv=SPLITTER):\n",
    "    grid_search = GridSearchCV(estimator=model,\n",
    "                               param_grid=params,\n",
    "                               n_jobs=-1, cv=cv,\n",
    "                               scoring='roc_auc',\n",
    "                               verbose=2)\n",
    "    \n",
    "    grid_search.fit(x_train, y_train)\n",
    "    print(f\"Best params for {model.__class__.__name__}: {grid_search.best_params_}\")\n",
    "    print(f\"Best Score: {grid_search.best_score_}\")\n",
    "    \n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "def train_model_random(model, params, cv=5):\n",
    "    random_search = RandomizedSearchCV(estimator=model,\n",
    "                                       param_grid=params,\n",
    "                                       n_jobs=-1,\n",
    "                                       n_iter=10,\n",
    "                                       cv=cv,\n",
    "                                       scoring='roc_auc',\n",
    "                                       verbose=2,\n",
    "                                       random_state=0)\n",
    "    \n",
    "    random_search.fit(x_train, y_train)\n",
    "    print(f\"Best params for {model.__class__.__name__}: {random_search.best_params_}\")\n",
    "    print(f\"Best Score: {random_search.best_score_}\")\n",
    "    \n",
    "    return random_search.best_estimator_\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4cad0d",
   "metadata": {},
   "source": [
    "# Evaluating a model to check for applicability\n",
    "\n",
    "With the model trained, we can check the results on the testing data and try to find out which one performs the best out of all of them. We will once again use the Area Under the Curve (AUC) as our metric, and we will show a Confusion Matrix to detail exactly what happenned with the data, and decide if it is a good fit or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "734a5ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be sure to use the X_test and Y_test from the train_test_split when doing this\n",
    "\n",
    "def evaluate(model):\n",
    "    y_pred = model.predict_proba(X_test)\n",
    "    y_pred = y_pred[:, -1]\n",
    "    \n",
    "    # Area Under the Curve, the higher the better\n",
    "    auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "    print(f\"AUC Score: {auc}\")\n",
    "\n",
    "    y_pred_normalized = np.argmax(model.predict_proba(X_test), axis=1)\n",
    "\n",
    "    cm = metrics.confusion_matrix(y_test, y_pred_normalized)\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n",
    "\n",
    "    # labels, title and ticks\n",
    "    ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "    ax.set_title('Confusion Matrix'); \n",
    "    ax.xaxis.set_ticklabels(['Accepted', 'Not Accepted']); ax.yaxis.set_ticklabels(['Accepted', 'Not Accepted']);\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11889db1",
   "metadata": {},
   "source": [
    "# Exporting the Data\n",
    "\n",
    "With the model trained, we can now look to apply it to the competition data as well, and (ideally) obtain great results with it as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57620935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The test_df is no more than the dataframe built with the competition data, so yeah, that should be easy\n",
    "\n",
    "def export_results(model, file):\n",
    "    test = test_df[features]\n",
    "\n",
    "    confidences = model.predict_proba(test)\n",
    "    confidences = confidences[:,-1]\n",
    "\n",
    "    confidences = [0 if x < 0.000001 else x for x in confidences]\n",
    "    confidences = [\"{:f}\".format(x) for x in confidences]\n",
    "\n",
    "    submition_data = pd.DataFrame()\n",
    "\n",
    "    submition_data[\"Id\"] = test_df[\"loan_id\"]\n",
    "    submition_data[\"Predicted\"] = confidences\n",
    "    submition_data.to_csv(f\"results/{file}.csv\", sep=\",\", index=False)\n",
    "    \n",
    "    print(f\"Done exporting to: {file}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb85976",
   "metadata": {},
   "source": [
    "# Joining it all together\n",
    "\n",
    "With this wrapper function, it becomes much easier to evaluate all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "935d3ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predicted results might be useful if we wish to compare the different models\n",
    "def apply(model, params):\n",
    "    best_model = train_model_grid(model, params)\n",
    "    predicted_results = evaluate(best_model)\n",
    "    export_results(model, model.__class__.__name__)\n",
    "    \n",
    "    return predicted_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bbfd8d",
   "metadata": {},
   "source": [
    "# TODO: Start applying models, with different params, and evaluating the results. Submit the best one. Also, don't forget to upload the data kkkkk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc32c96",
   "metadata": {},
   "source": [
    "# TODO(MAYBE): Add this ROC comparing thingy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394815ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def roc_curve_and_score(y_test, pred_proba):\n",
    "    fpr, tpr, _ = roc_curve(y_test.ravel(), pred_proba.ravel())\n",
    "    roc_auc = roc_auc_score(y_test.ravel(), pred_proba.ravel())\n",
    "    return fpr, tpr, roc_auc\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "matplotlib.rcParams.update({'font.size': 14})\n",
    "plt.grid()\n",
    "\n",
    "fpr, tpr, roc_auc = roc_curve_and_score(y_test, y_pred_rfc)\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='RFC({0:.3f})'.format(roc_auc))\n",
    "\n",
    "fpr, tpr, roc_auc = roc_curve_and_score(y_test, y_pred_svm)\n",
    "plt.plot(fpr, tpr, color='red', lw=2, label='SVM({0:.3f})'.format(roc_auc))\n",
    "\n",
    "fpr, tpr, roc_auc = roc_curve_and_score(y_test, y_pred_xgb)\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='XGB({0:.3f})'.format(roc_auc))\n",
    "\n",
    "fpr, tpr, roc_auc = roc_curve_and_score(y_test, y_pred_neural)\n",
    "plt.plot(fpr, tpr, color='purple', lw=2, label='NEURAL({0:.3f})'.format(roc_auc))\n",
    "\n",
    "fpr, tpr, roc_auc = roc_curve_and_score(y_test, y_pred_knn)\n",
    "plt.plot(fpr, tpr, color='crimson', lw=2, label='KNN({0:.3f})'.format(roc_auc))\n",
    "\n",
    "fpr, tpr, roc_auc = roc_curve_and_score(y_test, y_pred_dtc)\n",
    "plt.plot(fpr, tpr, color='yellow', lw=2, label='DTC({0:.3f})'.format(roc_auc))\n",
    "\n",
    "fpr, tpr, roc_auc = roc_curve_and_score(y_test, y_pred_gp)\n",
    "plt.plot(fpr, tpr, color='green', lw=2, label='GP({0:.3f})'.format(roc_auc))\n",
    "\n",
    "fpr, tpr, roc_auc = roc_curve_and_score(y_test, y_pred_mlp)\n",
    "plt.plot(fpr, tpr, color='black', lw=2, label='MLP({0:.3f})'.format(roc_auc))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('1 - Specificity')\n",
    "plt.ylabel('Sensitivity')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
